{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Hidden Layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script saves the hidden layer activations from the models in the /model folder.\n",
    "This script should be run from the same docker container as the models are trained from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import yaml, os, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nn_models import ConvModel, AffineModel, RecurrentModel\n",
    "from nn_rmodels import ConvRModel, RecurrentRModel\n",
    "from nn_train_utils import Dataset\n",
    "import matplotlib\n",
    "import pickle#, time\n",
    "from tensorflow.contrib.rnn import *\n",
    "from main import RunInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify model, run information, and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelinfo = dict({'type': 'S',\n",
    "        'typename': 'spatial_temporal',\n",
    "        'base': 'spatial_temporal_4_8-16-16-32_32-32-64-64_7293',\n",
    "        'base_regression': 'spatial_temporal_r_4_8-16-16-32_32-32-64-64_7293',\n",
    "        'nlayers': 8,\n",
    "        'max_nlayers': 8,\n",
    "        'max_act': 14, #this can be manually adjusted as the maximum in the preferred direction histogram\n",
    "        'control': False,\n",
    "        'cmap': matplotlib.colors.ListedColormap(['midnightblue']),\n",
    "        'color': 'midnightblue',\n",
    "        'regression_color': 'darkturquoise',\n",
    "        'control_cmap': 'Greys_r',\n",
    "        'regression_cmap': matplotlib.colors.ListedColormap(['darkturquoise']),\n",
    "        's_stride': 2,\n",
    "        't_stride': 3,\n",
    "        'regression_task': False,\n",
    "        'model_path': None,\n",
    "        'exp_id': None,})\n",
    "\n",
    "runinfo = RunInfo({'expid': 402, #internal experiment id\n",
    "                   'datafraction': 'auto',  #fraction (0,1] or 'auto' (i.e. if you want to run a new analysis but keep the old results that it would otherwise overwrite, increment by 1)\n",
    "                   'randomseed': 2000,\n",
    "                   'randomseed_traintest': 42,\n",
    "                   'dirr2threshold': 0.2,\n",
    "                   'verbose': 2, #0 (least), 1, 2 (most)\n",
    "                   'model_experiment_id': 22,  #used in model training, int or 'auto'\n",
    "                   'basefolder': '/media/data/DeepDraw/revisions/analysis-data/', ## change this folder to redirect to where the data is saved locally\n",
    "                   'batchsize': 100, #for layer representation generation\n",
    "                   'default_run': True, #only variable that is 'trial'-dependent,\n",
    "                                    #ie should be changed when rerunning stuff in same folder\n",
    "                                    #not semantically important for run info\n",
    "                    'dpi': 500\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinnames = ['endeffector_coords', 'joint_coords', 'muscle_coords', 'speed']\n",
    "\n",
    "basefolder = runinfo['basefolder']\n",
    "modelname = modelinfo['name']\n",
    "\n",
    "model_path = f\"{basefolder}models/experiment_{runinfo.model_experiment_id}/{modelname}/\"\n",
    "#path_to_data = f'{basefolder}../deep_proprioception/dataset/pcr_dataset_test.hdf5'\n",
    "path_to_data = f'{basefolder}../pcr_data/pcr_dataset_test.hdf5'\n",
    "PATH_TO_DATA = f'{basefolder}../pcr_data/'\n",
    "MODELS_DIR = basefolder\n",
    "path_to_config_file = f\"{basefolder}models/experiment_{runinfo.model_experiment_id}/{modelname}/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbase = modelinfo['base']\n",
    "datafraction = runinfo['datafraction']\n",
    "\n",
    "np.random.seed(runinfo['randomseed'])\n",
    "\n",
    "print(modelname)\n",
    "\n",
    "if path_to_data is not None:\n",
    "    with h5py.File(path_to_data, 'r') as datafile:\n",
    "        idxtups = []\n",
    "        shape = datafile[kinnames[0]][()].shape\n",
    "        kinarr = np.zeros((shape[0], 0, shape[2]))\n",
    "        for name in kinnames:\n",
    "\n",
    "            kin = datafile[name][()]\n",
    "            try:\n",
    "                ncols = datafile[name][()].shape[1]\n",
    "            except:\n",
    "                ncols = 1\n",
    "                kin = kin.reshape(-1,1)\n",
    "                kin = np.repeat(kin, shape[2], axis=1)\n",
    "                kin = kin.reshape(kin.shape[0], 1, kin.shape[1])\n",
    "            idxtups += list(zip([name]*ncols, range(ncols)))\n",
    "            kinarr = np.concatenate((kinarr, kin), axis=1)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(idxtups)\n",
    "\n",
    "#SPINDLE FIRING TEST DATA\n",
    "test_data_path = os.path.join(PATH_TO_DATA, 'pcr_dataset_test.hdf5')\n",
    "dataset = Dataset(test_data_path, dataset_type='test', key='spindle_info')    \n",
    "\n",
    "#Extract needed data\n",
    "data = dataset.test_data\n",
    "labels = dataset.test_labels\n",
    "\n",
    "# For when I want to use only a fraction of the dataset to train!\n",
    "if datafraction is not None:\n",
    "    random_idx = np.random.permutation(data.shape[0])\n",
    "    subset_num = int(datafraction * random_idx.size)\n",
    "    data = data[random_idx[:subset_num]]\n",
    "    labels = labels[random_idx[:subset_num]]\n",
    "    kinarr = kinarr[random_idx[:subset_num]]\n",
    "\n",
    "nsamples, ninputs, ntime, _ = data.shape\n",
    "#batch_size = nsamples\n",
    "batch_size = runinfo.batchsize\n",
    "#batch_size = 100 #can be updated based on GPU capacities for forward pass\n",
    "#batch_size = 25 #can be updated based on GPU capacities for forward pass\n",
    "num_steps = nsamples // batch_size\n",
    "\n",
    "# CREATE PANDAS PANEL\n",
    "print('kinarr shape', kinarr.shape)\n",
    "kinvars = pd.Panel(np.swapaxes(kinarr, 0, 1), items=idx)\n",
    "#time.sleep(10)\n",
    "\n",
    "# INITIALIZE MODEL\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with open(path_to_config_file, 'r') as myfile:\n",
    "    model_config = yaml.load(myfile)\n",
    "    train_mean = model_config['train_mean']\n",
    "\n",
    "\n",
    "if not modelinfo['regression_task']:\n",
    "    if (modelinfo['type'] in ['S', 'ST']):\n",
    "        model = ConvModel(model_config['experiment_id'], model_config['nclasses'], model_config['arch_type'], \\\n",
    "                        int(model_config['nlayers']), model_config['n_skernels'], model_config['n_tkernels'], \\\n",
    "                        int(model_config['s_kernelsize']), int(model_config['t_kernelsize']), int(model_config['s_stride']), \n",
    "                        int(model_config['t_stride']))\n",
    "    \n",
    "    else:        \n",
    "        print('building rec model')\n",
    "        model = RecurrentModel(model_config['experiment_id'], model_config['nclasses'], model_config['rec_blocktype'], \n",
    "                            int(model_config['n_recunits']), int(model_config['npplayers']), list(map(int, model_config['nppfilters'])), \n",
    "                            int(model_config['s_kernelsize']), int(model_config['s_stride']))\n",
    "\n",
    "else:\n",
    "    if (modelinfo['type'] in ['S', 'ST']):\n",
    "        model = ConvRModel(model_config['experiment_id'], model_config['arch_type'], \\\n",
    "                        int(model_config['nlayers']), model_config['n_skernels'], model_config['n_tkernels'], \\\n",
    "                        int(model_config['s_kernelsize']), int(model_config['t_kernelsize']), int(model_config['s_stride']), \n",
    "                        int(model_config['t_stride']), noutspace=6)\n",
    "    \n",
    "    else:        \n",
    "        print('building rec model')\n",
    "        model = RecurrentRModel(model_config['experiment_id'], model_config['rec_blocktype'], \n",
    "                            int(model_config['n_recunits']), int(model_config['npplayers']), list(map(int, model_config['nppfilters'])), \n",
    "                            int(model_config['s_kernelsize']), int(model_config['s_stride']), noutspace=6)\n",
    "\n",
    "\n",
    "print(\"Old model path: \", model.model_path)\n",
    "if not modelinfo['regression_task']:\n",
    "    model.model_path = basefolder + model.model_path\n",
    "    if(not modelinfo['control']):\n",
    "        model.model_path = model.model_path + modelname[-2:] #Add control set number\n",
    "    else:\n",
    "        model.model_path = model.model_path + modelname[-3:]\n",
    "else:\n",
    "    model.model_path = model_path\n",
    "print(\"New model path: \", model.model_path)\n",
    "\n",
    "print('Final model.model_path', model.model_path)\n",
    "\n",
    "#SAVE FOLLOW THROUGH    \n",
    "datafolder = os.get_cwd()#runinfo.datafolder(modelinfo)\n",
    "os.makedirs(datafolder, exist_ok=True)\n",
    "kinvars.to_hdf(datafolder + \"/kinvars.hdf5\", key=\"data\")\n",
    "print(\"Kinvars saved\")\n",
    "\n",
    "pickle.dump(data, open(datafolder + \"/data.pkl\", \"wb\"), protocol=4)\n",
    "print(\"MF saved\")\n",
    "\n",
    "pickle.dump(labels, open(datafolder + \"/labels.pkl\", \"wb\"), protocol=4)\n",
    "print(\"Labels saved\")\n",
    "\n",
    "#layers = []\n",
    "\n",
    "mygraph = tf.Graph()\n",
    "with mygraph.as_default():\n",
    "    # Declare placeholders for input data and labels\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size, ninputs, ntime, 2], name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size], name=\"y\")\n",
    "\n",
    "    # Compute scores and accuracy\n",
    "    if not modelinfo['regression_task']:\n",
    "        scores, probabilities, net = model.predict(X, is_training=False)\n",
    "    else:\n",
    "        scores, net = model.predict(X, is_training=False)\n",
    "\n",
    "    # Test the `model`!\n",
    "    restorer = tf.train.Saver()\n",
    "    myconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    \n",
    "    for j in range(len(list((net.values()))) - 1):\n",
    "        with tf.Session(config=myconfig) as sess:\n",
    "            ckpt_filepath = os.path.join(model.model_path, 'model.ckpt')\n",
    "            print('checkpoint filepath', ckpt_filepath)\n",
    "            restorer.restore(sess, ckpt_filepath)\n",
    "            \n",
    "            for i in range(num_steps):\n",
    "                if(runinfo.verbose >= 1):\n",
    "                    print('batch %d / %d' %(i, num_steps))\n",
    "                layer_batch = sess.run(list((net.values()))[j], \\\n",
    "                        feed_dict={X: data[batch_size*i:batch_size*(i+1)], y: labels[batch_size*i:batch_size*(i+1)]})\n",
    "                \n",
    "                if i == 0:\n",
    "                    layer = h5py.File(datafolder + f\"/l{j}.hdf5\", 'w')\n",
    "                    \n",
    "                layer.create_dataset(str(i), data=layer_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
