{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Hidden Layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script saves the hidden layer activations from the models in the /model folder.\n",
    "This script should be run from the same docker container as the models are trained from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kai/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import yaml, os, h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy import exp\n",
    "from nn_models import ConvModel, AffineModel, RecurrentModel\n",
    "from nn_train_utils import Dataset\n",
    "import pickle\n",
    "from controls_main import RunInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify model, run information, and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelinfo = dict({'type': 'S',\n",
    "            'base': 'spatial_temporal_4_8-16-16-32_64-64-64-64_5272',\n",
    "            'name': 'spatial_temporal_4_8-16-16-32_64-64-64-64_5272_1',\n",
    "            'shortbase': 'Spatial-Temporal',\n",
    "            'shortname': 'Spatial-Temporal_1',\n",
    "            'nlayers': 8,\n",
    "            'max_act': 14,\n",
    "            'control': False,\n",
    "            'cmap': 'Blues_r',\n",
    "            'color': 'C0',\n",
    "            'control_cmap': 'Purples_r'})\n",
    "\n",
    "runinfo = RunInfo({'expid': 102, #internal experiment id\n",
    "                   'datafraction': 0.5,\n",
    "                   'randomseed': 2000,\n",
    "                   'randomseed_traintest': 42,\n",
    "                   'dirr2threshold': 0.2,\n",
    "                   'verbose': 0,\n",
    "                   'model_experiment_id': 4, #as per Pranav's model generation\n",
    "                   'basefolder' : '/home/kai/Dropbox/DeepDrawData/analysis-data/', #point to analysis-data folder\n",
    "                       #in DeepDrawData folder from Dropbox, include trailing slash\n",
    "                   'orientation' : 'hor',\n",
    "                   'height' : 'all'\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinnames = ['endeffector_coords', 'joint_coords', 'muscle_coords', 'speed']\n",
    "\n",
    "basefolder = runinfo['basefolder']\n",
    "modelname = modelinfo['name']\n",
    "\n",
    "model_path = f\"{basefolder}models/experiment_{runinfo.model_experiment_id}/{modelname}/\"\n",
    "#path_to_data = f'{basefolder}../deep_proprioception/dataset/pcr_dataset_test.hdf5'\n",
    "path_to_data = f'{basefolder}../pcr_data/pcr_dataset_test.hdf5'\n",
    "PATH_TO_DATA = f'{basefolder}../pcr_data/'\n",
    "MODELS_DIR = basefolder\n",
    "path_to_config_file = f\"{basefolder}models/experiment_{runinfo.model_experiment_id}/{modelname}/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c6affc823c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0midxtups\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#speed = datafile[name][()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mkinarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkinarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m#jointcoords = datafile[name][()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelbase = modelinfo['base']\n",
    "datafraction = runinfo['datafraction']\n",
    "np.random.seed(runinfo['randomseed'])\n",
    "\n",
    "if path_to_data is not None:\n",
    "    with h5py.File(path_to_data, 'r') as datafile:\n",
    "        idxtups = []\n",
    "        shape = datafile[kinnames[0]][()].shape\n",
    "        kinarr = np.zeros((shape[0], 0, shape[2]))\n",
    "        for name in kinnames:\n",
    "\n",
    "            kin = datafile[name][()]\n",
    "            try:\n",
    "                ncols = datafile[name][()].shape[1]\n",
    "            except:\n",
    "                ncols = 1\n",
    "                kin = kin.reshape(-1,1)\n",
    "                kin = np.repeat(kin, shape[2], axis=1)\n",
    "                kin = kin.reshape(kin.shape[0], 1, kin.shape[1])\n",
    "            idxtups += list(zip([name]*ncols, range(ncols)))\n",
    "            #speed = datafile[name][()]\n",
    "            kinarr = np.concatenate((kinarr, kin), axis=1)\n",
    "            #jointcoords = datafile[name][()]\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(idxtups)\n",
    "\n",
    "#SPINDLE FIRING TEST DATA\n",
    "\n",
    "test_data_path = os.path.join(PATH_TO_DATA, 'pcr_dataset_test.hdf5')\n",
    "dataset = Dataset(test_data_path, dataset_type='test')\n",
    "\n",
    "#Extract needed data\n",
    "data = dataset.test_data\n",
    "labels = dataset.test_labels\n",
    "\n",
    "# For when I want to use only a fraction of the dataset to train!\n",
    "if datafraction is not None:\n",
    "    random_idx = np.random.permutation(data.shape[0])\n",
    "    subset_num = int(datafraction * random_idx.size)\n",
    "    data = data[random_idx[:subset_num]]\n",
    "    labels = labels[random_idx[:subset_num]]\n",
    "    kinarr = kinarr[random_idx[:subset_num]]\n",
    "\n",
    "nsamples, ninputs, ntime = data.shape\n",
    "#batch_size = 1\n",
    "batch_size = nsamples\n",
    "num_steps = nsamples // batch_size\n",
    "\n",
    "# CREATE PANDAS PANEL\n",
    "kinvars = pd.Panel(np.swapaxes(kinarr, 0, 1), items=idx)\n",
    "\n",
    "# INITIALIZE MODEL\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with open(path_to_config_file, 'r') as myfile:\n",
    "    model_config = yaml.load(myfile)\n",
    "    train_mean = model_config['train_mean']\n",
    "\n",
    "model = ConvModel(model_config['experiment_id'], model_config['nclasses'], model_config['arch_type'], \\\n",
    "                  int(model_config['nlayers']), model_config['n_skernels'], model_config['n_tkernels'], \\\n",
    "                  int(model_config['s_kernelsize']), int(model_config['t_kernelsize']), int(model_config['s_stride']), \n",
    "                  int(model_config['t_stride']))\n",
    "\n",
    "# RUN PREDICTIONS AND SAVE INFORMATION FOR TUNING CURVE\n",
    "mygraph = tf.Graph()\n",
    "with mygraph.as_default():\n",
    "\n",
    "    ##BUILD GRAPH\n",
    "    # Declare placeholders for input data and labels\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size, ninputs, ntime], name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size], name=\"y\")\n",
    "\n",
    "    # Compute scores and accuracy\n",
    "    scores, probabilities, net = model.predict(X, is_training=False)\n",
    "    correct = tf.nn.in_top_k(probabilities, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")    \n",
    "\n",
    "    # Build layer output reader\n",
    "    #layers = [np.zeros(layer.get_shape().as_list()) for layer in list(net.values())]\n",
    "\n",
    "    ##PROBE RECEPTIVE FIELDS\n",
    "\n",
    "    if(not modelinfo['control']):\n",
    "        model.model_path = basefolder + model.model_path + modelname[-2:] #Add control set number\n",
    "    else:\n",
    "        model.model_path = basefolder + model.model_path + modelname[-3:]\n",
    "    restorer = tf.train.Saver()\n",
    "    myconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    with tf.Session(config=myconfig) as sess:\n",
    "        ckpt_filepath = os.path.join(model.model_path, 'model.ckpt')\n",
    "        restorer.restore(sess, ckpt_filepath)\n",
    "        layers = sess.run(list((net.values())), feed_dict={X: data, y: labels})\n",
    "\n",
    "#SAVE FOLLOW THROUGH\n",
    "\n",
    "datafolder = os.path.join(runinfo.datafolder(modelinfo), 'demo_notebook')\n",
    "os.makedirs(datafolder, exist_ok=True)\n",
    "#os.mkdir(datafolder)\n",
    "kinvars.to_hdf(datafolder + \"/kinvars.hdf5\", key=\"data\")\n",
    "print(\"Kinvars saved\")\n",
    "\n",
    "pickle.dump(data, open(datafolder + \"/data.pkl\", \"wb\"))\n",
    "print(\"MF saved\")\n",
    "\n",
    "pickle.dump(labels, open(datafolder + \"/labels.pkl\", \"wb\"))\n",
    "print(\"Labels saved\")\n",
    "\n",
    "for i in range(len(layers) - 1):\n",
    "    pickle.dump(layers[i], open(datafolder + f\"/l{i}.pkl\", \"wb\"))\n",
    "    print(f\"L{i} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
