{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1. Make subset of 200000 trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "PATH_TO_DATA = '/gpfs01/bethge/home/pmamidanna/deep_proprioception/data/'\n",
    "PATH_TO_TEMPDATA = '/gpfs01/bethge/home/pmamidanna/deep_proprioception/data/unprocessed_data/'\n",
    "\n",
    "files_list = glob.glob(os.path.join(PATH_TO_TEMPDATA, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Latents = namedtuple('Latents', 'size rot shear_x shear_y speed noise')\n",
    "all_jobs = [pickle.load(open(fname, 'rb')) for fname in files_list if 'vertical_inv' not in fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datapoints = [datapoint for job in all_jobs for datapoint in job]\n",
    "\n",
    "for datapoint in all_datapoints:\n",
    "    datapoint['startpt'] = tuple(datapoint['startpt']) \n",
    "\n",
    "data = pd.DataFrame(all_datapoints)\n",
    "\n",
    "subset = []\n",
    "for label in range(1, 21):\n",
    "    for plane in ['horizontal',  'vertical']:\n",
    "        temp = data[data['label'] == label]\n",
    "        temp = temp[temp['plane'] == plane]\n",
    "        temp = temp.sort_values('muscle_jerk')\n",
    "        subset.append(temp.iloc[:5050])\n",
    "\n",
    "subset = pd.concat(subset)\n",
    "subset[['size', 'rot', 'shear_x', 'shear_y', 'speed', 'noise']] = subset['latents'].apply(pd.Series)\n",
    "subset = subset.drop(columns='latents')\n",
    "subset = subset.drop_duplicates(\n",
    "    subset=['label', 'startpt', 'size', 'rot', 'shear_x', 'shear_y', 'speed', 'noise', 'plane'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endeff = subset['endeffector_coords'] .tolist()\n",
    "joint = subset['joint_coords'].tolist()\n",
    "muscle = subset['muscle_coords'].tolist()\n",
    "\n",
    "labels = subset['label'].tolist()\n",
    "plane = subset['plane'].tolist()\n",
    "startpt = subset['startpt'].tolist()\n",
    "size = subset['size'].tolist()\n",
    "rot = subset['rot'].tolist()\n",
    "shear_x = subset['shear_x'].tolist()\n",
    "shear_y = subset['shear_y'].tolist()\n",
    "speed = subset['speed'].tolist()\n",
    "noise = subset['noise'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_nan(traj, max_length):\n",
    "    \"\"\"Pad nans to character trajectories that take less time.\"\"\"\n",
    "    num_dim = traj.shape[0]\n",
    "    new_traj = np.zeros((num_dim, max_length))\n",
    "    new_traj[:, :traj.shape[1]] = traj\n",
    "    new_traj[:, traj.shape[1]:] = np.nan\n",
    "    return new_traj\n",
    "\n",
    "endeff = np.array([pad_nan(traj, 295) for traj in endeff])\n",
    "joint = np.array([pad_nan(traj, 295) for traj in joint])\n",
    "muscle = np.array([pad_nan(traj, 295) for traj in muscle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('./data/pcr_full_temp.hdf5', 'w') as myfile:\n",
    "    myfile.create_dataset('endeffector_coords', data=endeff)\n",
    "    myfile.create_dataset('joint_coords', data=joint)\n",
    "    myfile.create_dataset('muscle_coords', data=muscle)\n",
    "    myfile.create_dataset('label', data=labels)\n",
    "    myfile.create_dataset('plane', data=np.array(plane, dtype='S'))\n",
    "    myfile.create_dataset('startpt', data=startpt)\n",
    "    myfile.create_dataset('size', data=size)\n",
    "    myfile.create_dataset('rot', data=rot)\n",
    "    myfile.create_dataset('shear_x', data=shear_x)\n",
    "    myfile.create_dataset('shear_y', data=shear_y)\n",
    "    myfile.create_dataset('speed', data=speed)\n",
    "    myfile.create_dataset('noise', data=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot muscle jerks classwise!\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=[14, 5])\n",
    "sns.boxplot(x='label', y='muscle_jerk', hue='plane', data=subset)\n",
    "charlabels = ['a', 'b', 'c', 'd', 'e', 'g', 'h', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 'u', 'v', 'w', 'y', 'z']\n",
    "plt.xticks(np.arange(20), charlabels)\n",
    "sns.despine(trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2. Generate Spindle trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rearranging muscles.**  \n",
    "\n",
    "Date : 30th November.  \n",
    "\n",
    "Previously, the arrangement of the muscles in the muscle configurations were according to alphabetical order. Now, I will convert into an order that is still alphabetical, but arranges shoulder muscles and then elbow muscles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with h5py.File('./data/pcr_full_temp.hdf5', 'r') as f:\n",
    "    endeffector_coords = f['endeffector_coords'][()]\n",
    "    joint_coords = f['joint_coords'][()]\n",
    "    muscle_coords = f['muscle_coords'][()]\n",
    "    labels = f['label'][()]\n",
    "    planes = f['plane'][()]\n",
    "    startpts = f['startpt'][()]\n",
    "    size = f['size'][()]\n",
    "    rot = f['rot'][()]\n",
    "    shear_x = f['shear_x'][()]\n",
    "    shear_y = f['shear_y'][()]\n",
    "    speed = f['speed'][()]\n",
    "    noise = f['noise'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_order = ['ANC', 'BIClong', 'BICshort', 'BRA', 'BRD', 'CORB', 'DELT1', 'DELT2', 'DELT3',\n",
    "             'ECRL', 'INFSP', 'LAT1', 'LAT2', 'LAT3', 'PECM1', 'PECM2', 'PECM3', 'PT', 'SUBSC', \n",
    "             'SUPSP','TMAJ', 'TMIN', 'TRIlat', 'TRIlong', 'TRImed']\n",
    "new_order = ['CORB', 'DELT1', 'DELT2', 'DELT3', 'INFSP', 'LAT1', 'LAT2', 'LAT3', 'PECM1',\n",
    "             'PECM2', 'PECM3', 'SUBSC', 'SUPSP', 'TMAJ', 'TMIN', 'ANC', 'BIClong', 'BICshort',\n",
    "             'BRA', 'BRD', 'ECRL', 'PT', 'TRIlat', 'TRIlong', 'TRImed']\n",
    "\n",
    "new_to_old = np.argsort(new_order)\n",
    "old_to_new = np.argsort(new_to_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_muscle_coords = np.zeros_like(muscle_coords)\n",
    "for i in range(new_muscle_coords.shape[0]):\n",
    "    new_muscle_coords[i] = muscle_coords[i] if planes[i] == b'vertical' else muscle_coords[i, old_to_new, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test!!\n",
    "\n",
    "import opensim as osim\n",
    "PATH_TO_OSIM_MODEL = '/gpfs01/bethge/home/pmamidanna/deep_proprioception/data/dynamic_arm_model/'\n",
    "model = osim.Model(os.path.join(PATH_TO_OSIM_MODEL, 'MoBL_ARMS_module5_scaleIK.osim'))\n",
    "\n",
    "import sys\n",
    "sys.path.append('/gpfs01/bethge/home/pmamidanna/deep_proprioception/code/')\n",
    "from pcr_data_utils import make_muscle_config\n",
    "\n",
    "rand_idx = np.random.permutation(joint_coords.shape[0])[:10]\n",
    "\n",
    "sample_joint_coords = [joint_coords[rand_id][:, np.all(~np.isnan(joint_coords[rand_id]), axis=0)]\n",
    "                       for rand_id in rand_idx]\n",
    "sample_muscle_coords_old = [muscle_coords[rand_id][:, np.all(~np.isnan(muscle_coords[rand_id]), axis=0)]\n",
    "                            for rand_id in rand_idx]\n",
    "sample_muscle_coords_new = [new_muscle_coords[rand_id][:, np.all(~np.isnan(new_muscle_coords[rand_id]), axis=0)]\n",
    "                            for rand_id in rand_idx]\n",
    "\n",
    "test_shuffling = []\n",
    "for i in range(10):\n",
    "    test_shuffling.append(make_muscle_config(model, sample_joint_coords[i]))\n",
    "\n",
    "print('Difference between regenerated test samples and old muscle coordinates.')\n",
    "print([np.linalg.norm(sample_muscle_coords_old[i] - test_shuffling[i]) for i in range(10)])\n",
    "print('Difference between regenerated test samples and new muscle coordinates')\n",
    "print([np.linalg.norm(sample_muscle_coords_new[i] - test_shuffling[i]) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute fiber lengths instead of normalized fiber lengths.**  \n",
    "\n",
    "Date: 6th December 2018\n",
    "\n",
    "Look at the notebook `OptimalFiberLength` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_optimalfiberlengths(model):\n",
    "    init_state = model.initSystem()\n",
    "    model.equilibrateMuscles(init_state)\n",
    "\n",
    "    # Prepare for simulation\n",
    "    muscle_set = model.getMuscles() # returns a Set<Muscles> object\n",
    "    num_muscles = muscle_set.getSize()\n",
    "\n",
    "    optimalfiberlengths = {}\n",
    "    for i in range(num_muscles):\n",
    "        optimalfiberlengths[muscle_set.get(i).getName()] = muscle_set.get(i).getOptimalFiberLength()\n",
    "\n",
    "    return optimalfiberlengths\n",
    "\n",
    "optimal_fiber_lengths = collect_optimalfiberlengths(model)\n",
    "optimal_length_multiplier = np.zeros(len(new_order))\n",
    "for i in range(len(new_order)):\n",
    "    optimal_length_multiplier[i] = optimal_fiber_lengths[new_order[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_muscle_coords *= optimal_length_multiplier[None, :, None]\n",
    "new_muscle_coords *= 1000 # Convert muscle length from m to mm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Spindle Responses using the Prochazka Gorassini 1998 Model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signpow(a,b): return np.sign(a)*(np.abs(a)**b)\n",
    "\n",
    "def make_spindle_coords(muscle_traj):\n",
    "    stretch = np.gradient(muscle_traj, 1, axis=1)\n",
    "    stretch_vel = np.gradient(muscle_traj, 0.015, axis=1)\n",
    "    p_rate = 2*stretch + 4.3*signpow(stretch_vel, 0.6)\n",
    "    return p_rate\n",
    "\n",
    "def start_end_choice(traj):\n",
    "    true_traj = traj[:, np.all(~np.isnan(traj), axis=0)]\n",
    "    room = 320 - true_traj.shape[1]\n",
    "    start_idx = np.random.randint(room)\n",
    "    end_idx = start_idx + true_traj.shape[1]\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def apply_shuffle(traj, start_idx, end_idx):\n",
    "    true_traj = traj[:, np.all(~np.isnan(traj), axis=0)]\n",
    "    mytraj = np.zeros((true_traj.shape[0], 320))\n",
    "    mytraj[:, start_idx:end_idx] = true_traj\n",
    "    mytraj[:, :start_idx] = true_traj[:, 0][:, None]\n",
    "    mytraj[:, end_idx:] = true_traj[:, -1][:, None]\n",
    "    return mytraj\n",
    "\n",
    "def add_noise(mconf, factor):\n",
    "    noisy_mconf = mconf + factor*mconf.std(axis=1)[:, None]*np.random.randn(*mconf.shape)\n",
    "    return noisy_mconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = endeffector_coords.shape[0]\n",
    "endeffector_coords_new = np.zeros((nsamples, 3, 320))\n",
    "muscle_coords_new = np.zeros((nsamples, 25, 320))\n",
    "joint_coords_new = np.zeros((nsamples, 4, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = []\n",
    "end_id = []\n",
    "for i in range(nsamples):\n",
    "    start_idx, end_idx = start_end_choice(endeffector_coords[i])\n",
    "    endeffector_coords_new[i] = apply_shuffle(endeffector_coords[i], start_idx, end_idx)\n",
    "    joint_coords_new[i] = apply_shuffle(joint_coords[i], start_idx, end_idx)\n",
    "    muscle_coords_new[i] = apply_shuffle(new_muscle_coords[i], start_idx, end_idx)\n",
    "    start_id.append(start_idx); end_id.append(end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spindle_firing = np.zeros_like(muscle_coords_new)\n",
    "for i in range(nsamples):\n",
    "    temp = make_spindle_coords(muscle_coords_new[i])\n",
    "    spindle_firing[i] = add_noise(temp, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Datasets!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test splits.\n",
    "num_examples = spindle_firing.shape[0]\n",
    "shuffle_idx = np.random.RandomState(seed=12).permutation(num_examples)\n",
    "start_id = np.array(start_id)\n",
    "end_id = np.array(end_id)\n",
    "num_train = int(0.8*200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/pcr_dataset_train.hdf5', 'w') as myfile:\n",
    "    myfile.create_dataset('endeffector_coords', data=endeffector_coords_new[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('joint_coords', data=joint_coords_new[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('muscle_coords', data=muscle_coords_new[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('spindle_firing', data=spindle_firing[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('label', data=labels[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('plane', data=planes[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('startpt', data=startpts[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('size', data=size[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('rot', data=rot[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('shear_x', data=shear_x[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('shear_y', data=shear_y[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('speed', data=speed[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('start_id', data=start_id[shuffle_idx[:num_train]])\n",
    "    myfile.create_dataset('end_id', data=end_id[shuffle_idx[:num_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/pcr_dataset_test.hdf5', 'w') as myfile:\n",
    "    myfile.create_dataset('endeffector_coords', data=endeffector_coords_new[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('joint_coords', data=joint_coords_new[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('muscle_coords', data=muscle_coords_new[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('spindle_firing', data=spindle_firing[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('label', data=labels[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('plane', data=planes[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('startpt', data=startpts[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('size', data=size[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('rot', data=rot[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('shear_x', data=shear_x[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('shear_y', data=shear_y[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('speed', data=speed[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('start_id', data=start_id[shuffle_idx[num_train:200000]])\n",
    "    myfile.create_dataset('end_id', data=end_id[shuffle_idx[num_train:200000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('./data/pcr_dataset_diy.hdf5', 'w') as myfile:\n",
    "    myfile.create_dataset('endeffector_coords', data=endeffector_coords_new[shuffle_idx[:200000]], compression=9)\n",
    "    myfile.create_dataset('label', data=labels[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('plane', data=planes[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('startpt', data=startpts[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('size', data=size[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('rot', data=rot[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('shear_x', data=shear_x[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('shear_y', data=shear_y[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('speed', data=speed[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('noise', data=noise[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('start_id', data=start_id[shuffle_idx[:200000]])\n",
    "    myfile.create_dataset('end_id', data=end_id[shuffle_idx[:200000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "275px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
